{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic fundamentals of web scraping</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these two modules bs4 for selecting HTML tags easily\n",
    "from bs4 import BeautifulSoup\n",
    "# requests module is easy to operate some people use urllib but I prefer this one because it is easy to use.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scrap data from wikipedia</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Coronavirus - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "wiki=requests.get(\"https://en.wikipedia.org/wiki/Coronavirus\")\n",
    "soup=BeautifulSoup(wiki.text,'html')\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Find html tags with classes<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "\n",
      "1 Etymology\n",
      "2 History\n",
      "3 Microbiology\n",
      "\n",
      "3.1 Structure\n",
      "3.2 Genome\n",
      "3.3 Replication cycle\n",
      "\n",
      "3.3.1 Cell entry\n",
      "3.3.2 Genome translation\n",
      "3.3.3 Replicase-transcriptase\n",
      "3.3.4 Assembly and release\n",
      "\n",
      "\n",
      "3.4 Transmission\n",
      "\n",
      "\n",
      "4 Classification\n",
      "5 Origin\n",
      "6 Infection in humans\n",
      "\n",
      "6.1 Common cold\n",
      "6.2 Severe acute respiratory syndrome (SARS)\n",
      "6.3 Middle East respiratory syndrome (MERS)\n",
      "6.4 Coronavirus disease 2019 (COVID-19)\n",
      "\n",
      "\n",
      "7 Infection in animals\n",
      "\n",
      "7.1 Farm animals\n",
      "7.2 Domestic pets\n",
      "7.3 Laboratory animals\n",
      "\n",
      "\n",
      "8 Prevention and treatment\n",
      "9 See also\n",
      "10 References\n",
      "11 Further reading\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ww2_contents=soup.find_all(\"div\",class_='toc')\n",
    "for i in ww2_contents:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: Animal virologyCoronaviridaeHidden categories: CS1 French-language sources (fr)Webarchive template wayback linksArticles with short descriptionShort description is different from WikidataWikipedia extended-confirmed-protected pagesUse dmy dates from March 2020Articles with 'species' microformatsArticles containing Ancient Greek (to 1453)-language textAll articles with unsourced statementsArticles with unsourced statements from July 2020Commons category link is on WikidataCS1: long volume valueTaxonbars desynced from WikidataTaxonbars using multiple manual Wikidata itemsTaxonbars on possible non-taxon pagesWikipedia articles with GND identifiers\n"
     ]
    }
   ],
   "source": [
    "ww2_contents=soup.find_all(\"div\",class_='catlinks')\n",
    "for i in ww2_contents:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Findall operation in Bs4</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total links in my website : 1981\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "<a href=\"/wiki/Wikipedia:Protection_policy#extended\" title=\"This article is extended-confirmed protected\"><img alt=\"Extended-protected article\" data-file-height=\"512\" data-file-width=\"512\" decoding=\"async\" height=\"20\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/30px-Extended-protection-shackle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/40px-Extended-protection-shackle.svg.png 2x\" width=\"20\"/></a>\n",
      "<a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n",
      "<a class=\"mw-jump-link\" href=\"#searchInput\">Jump to search</a>\n",
      "<a href=\"/wiki/COVID-19_pandemic\" title=\"COVID-19 pandemic\">COVID-19 pandemic</a>\n",
      "<a href=\"/wiki/Coronavirus_disease_2019\" title=\"Coronavirus disease 2019\">Coronavirus disease 2019</a>\n"
     ]
    }
   ],
   "source": [
    "# findall function is used to fetch all tags at a single time.\n",
    "many_link=soup.find_all('a') # here i extracted all the anchor tags of my website\n",
    "total_links=len(many_link) # len function is use to calculate length of your array\n",
    "print(\"total links in my website :\",total_links)\n",
    "print()\n",
    "for i in many_link[:6]: # here i use slicing to fetch only first 6 links from rest of them.\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
